seed_everything: 42

model:
  class_path: src.models.jsa.JSA
  init_args:
    joint_model:
      class_path: src.models.components.joint_model.JointModelCategoricalGaussian
      init_args:
        num_categories: [1024]
        num_latent_vars: 1
        embedding_dims: [64]
        net:
          class_path: src.models.components.networks.ConvDecoder
          init_args:
            final_activation: tanh
            ch: 64 
            out_ch: 3
            ch_mult: [1, 2, 4]
            num_res_blocks: 2
            attn_resolutions: []
            activation: swish
            norm_type: group
            dropout: 0.0
            resamp_with_conv: true
            in_channels: null # not used in decoder
            resolution: 32
            z_channels: 64 # should calculated based on embedding_dims and num_latent_vars
    proposal_model:
      class_path: src.models.components.proposal_model.ProposalModelCategorical
      init_args:
        num_categories: [1024]
        num_latent_vars: 1
        net:
          class_path: src.models.components.networks.ConvEncoder
          init_args:
            ch: 64
            out_ch: null # not used in encoder
            ch_mult: [1, 2, 4]
            num_res_blocks: 2
            attn_resolutions: []
            activation: swish
            norm_type: group
            dropout: 0.0
            resamp_with_conv: true
            in_channels: 3
            resolution: 32
            z_channels: 1024 # should calculated based on num_categories and num_latent_vars
    sampler:
      _target_: src.samplers.misampler.MISampler
      use_cache: false # initial value, will be updated during training
      dataset_size: 50000
      element_shape: [8, 8] # height and width of latent variable
    gan_loss: null
      # class_path: src.models.components.losses.JSAGANLoss
      # init_args:
      #   disc_start: 250 # steps
      #   disc_num_layers: 3 # of conv layers
      #   disc_in_channels: 3
      #   disc_factor: 1.0
      #   disc_weight: 1.0
      #   disc_loss: hinge
    
    lr_joint: 3e-4
    lr_proposal: 3e-4
    lr_discriminator: 1e-4
    num_mis_steps: 20
    cache_start_epoch: 200

    init_from_ckpt: null
    init_mode: resume # "resume" or "warm_start"
    init_strict: false

    sigma_controller:
      _target_: src.utils.controllers.SigmaController
      mode: scheduled+adaptive
      init_sigma: 1.0
      clamp_to_schedule: true

      scheduler:
        _target_: src.utils.schedulers.SigmaScheduler
        max_val: 1.0
        min_val: 0.01
        warmup_steps: 5000
        total_steps: ${trainer.max_steps} # if using epochs, make sure to set max_epochs in trainer
        mode: cosine

      adaptive_controller:
        _target_: src.utils.controllers.ParameterController
        param_name: mis_sigma
        target_range: [0.2, 0.5]
        adjustment_rate: 0.01
        mode: exponential
     
      
  

data:
  class_path: src.data.cifar10.CIFAR10DataModule
  init_args:
    root: ./data/cifar10
    batch_size: 450
    num_workers: 4

trainer:
  logger:
    class_path: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: ./egs/continuous_cifar10
      name: categorical_prior_conv

  callbacks:
    # - class_path: src.utils.redirect_print_callback.RedirectPrintCallback
    #   init_args:
    #     filename: training.log
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: valid/nll
        mode: min
        save_top_k: 1
        filename: best-checkpoint
        save_last: true
    # - class_path: lightning.pytorch.callbacks.EarlyStopping # 不使用早停
    #   init_args:
    #     monitor: valid/nll
    #     mode: min
    #     patience: 10

    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: -1 # save all checkpoints
        filename: last-checkpoint
        every_n_epochs: 1
        dirpath: ./egs/continuous_mnist/categorical_prior/version_3/checkpoints
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: 2
    - class_path: src.utils.grad_norm_callback.GradientNormCallback
      init_args:
        norm_type: 2
        log_every_n_steps: 1

  max_epochs: 400
  max_steps: 5000 # higher priority than max_epochs
  accelerator: gpu
  devices: [4, 5, 6, 7]
  strategy: ddp_find_unused_parameters_true
  # strategy: auto
  precision: 32
  enable_progress_bar: true
  log_every_n_steps: 25
